{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "X3GaCwmTqFRL",
        "outputId": "0b184e18-1855-456a-b194-a82a91211a49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_maze', 'act', 'draw_env', 'free_cells', 'game_status', 'get_reward', 'maze', 'min_reward', 'observe', 'pirate', 'reset', 'state', 'target', 'total_reward', 'update_state', 'valid_actions', 'visited']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'TreasureMaze' object has no attribute 'take_action'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-27d322ddba58>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ✅ Replace with correct function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TreasureMaze' object has no attribute 'take_action'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-27d322ddba58>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ✅ Replace with correct function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move the pirate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get the reward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'TreasureMaze' object has no attribute 'take_action'"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Input\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# Ensure the extracted folder is added to the system path\n",
        "sys.path.append('/content/TreasureHuntGame')  # Update this path if necessary\n",
        "\n",
        "# Import game environment classes\n",
        "from TreasureMaze import TreasureMaze\n",
        "from GameExperience import GameExperience\n",
        "\n",
        "# Define the 8x8 maze\n",
        "maze = np.array([\n",
        "    [1., 0., 1., 1., 1., 1., 1., 1.],\n",
        "    [1., 0., 1., 1., 1., 0., 1., 1.],\n",
        "    [1., 1., 1., 1., 0., 1., 0., 1.],\n",
        "    [1., 1., 1., 0., 1., 1., 1., 1.],\n",
        "    [1., 1., 0., 1., 1., 1., 1., 1.],\n",
        "    [1., 1., 1., 0., 1., 0., 0., 0.],\n",
        "    [1., 1., 1., 0., 1., 1., 1., 1.],\n",
        "    [1., 1., 1., 1., 0., 1., 1., 1.]\n",
        "])\n",
        "\n",
        "# Initialize environment with the maze\n",
        "env = TreasureMaze(maze)\n",
        "\n",
        "# Print available methods to find the correct movement function\n",
        "print(dir(env))  # Run this once to find the correct method\n",
        "\n",
        "# Define state and action sizes if not provided by the environment\n",
        "try:\n",
        "    state_size = env.state_size\n",
        "    action_size = env.action_size\n",
        "except AttributeError:\n",
        "    state_size = 64  # Assuming an 8x8 flattened grid\n",
        "    action_size = 4  # Up, Down, Left, Right\n",
        "\n",
        "# Build the deep Q-learning model\n",
        "def build_model():\n",
        "    model = Sequential([\n",
        "        Input(shape=(state_size,)),  # ✅ Fix: Use Input() instead of input_dim\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(24, activation='relu'),\n",
        "        Dense(action_size, activation='linear')\n",
        "    ])\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))\n",
        "    return model\n",
        "\n",
        "model = build_model()\n",
        "\n",
        "# ✅ Fix: Pass the model when creating GameExperience\n",
        "memory = GameExperience(model)\n",
        "\n",
        "# Epsilon-greedy action selection\n",
        "def choose_action(state, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return random.choice(env.valid_actions())\n",
        "    state_input = np.array(state).reshape(1, -1)\n",
        "    q_values = model.predict(state_input, verbose=0)\n",
        "    return np.argmax(q_values[0])\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    state = env.reset(pirate=(0, 0))  # ✅ Fix: Pass required pirate argument\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "\n",
        "    while not done:\n",
        "        action = choose_action(state, epsilon)\n",
        "\n",
        "        # ✅ Fix: Use the correct method to move the pirate\n",
        "        try:\n",
        "            next_state, reward, done = env.take_action(action)  # ✅ Replace with correct function\n",
        "        except AttributeError:\n",
        "            env.take_action(action)  # Move the pirate\n",
        "            next_state = env.get_state()  # Get the new state\n",
        "            reward = env.get_reward()  # Get the reward\n",
        "            done = env.is_done()  # Check if the game is over\n",
        "\n",
        "        total_reward += reward\n",
        "\n",
        "        # Ensure memory is added in correct format\n",
        "        memory.add((state, action, reward, next_state, done))\n",
        "        state = next_state\n",
        "\n",
        "        if len(memory) >= batch_size:\n",
        "            batch = memory.sample(batch_size)\n",
        "            states, actions, rewards, next_states, dones = zip(*batch)\n",
        "\n",
        "            # Convert lists to NumPy arrays\n",
        "            states = np.array(states)\n",
        "            next_states = np.array(next_states)\n",
        "            rewards = np.array(rewards)\n",
        "            dones = np.array(dones)\n",
        "            actions = np.array(actions)\n",
        "\n",
        "            # Predict Q-values\n",
        "            q_next = model.predict(next_states, verbose=0)\n",
        "            targets = model.predict(states, verbose=0)\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                targets[i][actions[i]] = rewards[i] if dones[i] else rewards[i] + gamma * np.max(q_next[i])\n",
        "\n",
        "            model.train_on_batch(states, targets)\n",
        "\n",
        "    # Update epsilon\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "    print(f\"Epoch: {epoch}  Total Reward: {total_reward}  Epsilon: {epsilon}\")\n",
        "\n",
        "# Test the trained agent\n",
        "def play_game():\n",
        "    state = env.reset(pirate=(0, 0))  # ✅ Fix: Pass required pirate argument\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = choose_action(state, epsilon_min)  # Use lowest epsilon for best learned behavior\n",
        "\n",
        "        # ✅ Fix: Use the correct method to move the pirate\n",
        "        try:\n",
        "            state, _, done = env.take_action(action)\n",
        "        except AttributeError:\n",
        "            env.take_action(action)\n",
        "            state = env.get_state()\n",
        "            done = env.is_done()\n",
        "\n",
        "        env.render()\n",
        "    print(\"Game Over\")\n",
        "\n",
        "play_game()\n"
      ]
    }
  ]
}